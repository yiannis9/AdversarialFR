{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Msc Thesis\n",
    "\n",
    "Building an FR model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## LIBRARY IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "from mtcnn.mtcnn import MTCNN\n",
    "import cv2\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## LOAD DATASET AND PREPROCESS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "A few preprocessing steps are required to make the dataset ready for training\n",
    "\n",
    "- Extrace faces\n",
    "- Replace images with the extracted faces (This is common and much more efficient than including the whole image, since it can include many unimportant features)\n",
    "- Rescale the images to a smaller size (also a matter of efficiency. It is handled by the face extraction function formulated below)\n",
    "- Saving the train and test sets after preprocessing as numpy objects to enable easy reproduction of experiments in the future by just loading the numpy object.\n",
    "- Creating batch organised datasets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Face Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# can detect multiple faces but for our tasks we assume just one face images only\n",
    "def extract_faces_from_img(imagePath, required_size=(160, 160)):\n",
    "    image = Image.open(imagePath)\n",
    "    image = np.asarray(image)\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    faceCascade = cv2.CascadeClassifier(cv2.data.haarcascades + \"haarcascade_frontalface_default.xml\")\n",
    "    faces = faceCascade.detectMultiScale(\n",
    "                                        gray,\n",
    "                                        scaleFactor=1.3,\n",
    "                                        minNeighbors=3,\n",
    "                                        minSize=(30, 30)\n",
    "                                        )\n",
    "    if len(faces) != 0:\n",
    "        for (x, y, w, h) in faces:\n",
    "            cv2.rectangle(image, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "            roi_color = image[y:y + h, x:x + w]\n",
    "        # resize pixels to the model size\n",
    "        image = Image.fromarray(roi_color)\n",
    "        image = image.resize(required_size)\n",
    "        face_array = np.asarray(image)\n",
    "    else:\n",
    "        # if face is not detected we need to return None\n",
    "        face_array = None\n",
    "    return face_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom 6-person dataset for a classiffier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "samples = {\n",
    "            'n002462':'Elizabeth Olsen',\n",
    "            'n002038':'David Hasselhoff',\n",
    "            'n000419':'Amy Adams',\n",
    "            'n000709':'Antonio Banderas',\n",
    "            'n001063':'Billy Idol',\n",
    "            'n003862':'Jason Derulo'\n",
    "}\n",
    "\n",
    "train_ds_path = r'C:\\Users\\Keravnos\\Documents\\VGG-Face2\\data\\vggface2_train.tar\\train'\n",
    "\n",
    "def prepare_dataset(samples, train_data_path):\n",
    "    ds = []\n",
    "    # list out dict keys\n",
    "    key_list = list(samples.keys())\n",
    "    #iterate over our 6 identity directories\n",
    "    for k in tqdm(key_list,'Identities'):\n",
    "        # use this as label\n",
    "        label = samples[k]\n",
    "        #directory path of images for each identity\n",
    "        dir_path = os.path.join(train_data_path,k)\n",
    "        print('Extracting images for', label)\n",
    "        # iterate over image directory and extract faces\n",
    "        for img in tqdm(os.listdir(dir_path),'Images'):\n",
    "            # need the image path for the face extraction method\n",
    "            img_path = os.path.join(dir_path,img)\n",
    "            # create a face array variable where the extracted face is constructed by the function.\n",
    "            face_array = extract_faces_from_img(img_path)\n",
    "            # add both image and label in dataset\n",
    "            ds.append((face_array,label))\n",
    "    print('DATASET COMPILED!')\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6eb39b6e525c4709a3ba0d524fa8f6d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Identities:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting images for Elizabeth Olsen\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a442994937e445db95b245d9348e904",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Images:   0%|          | 0/559 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting images for David Hasselhoff\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab04c205301e4582bd84e6d4d212b31f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Images:   0%|          | 0/542 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting images for Amy Adams\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34ac9632e6724d90976ba0ee69d6471f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Images:   0%|          | 0/689 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting images for Antonio Banderas\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fcc7a7a1774b4192b297f49c5be0b3fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Images:   0%|          | 0/451 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting images for Billy Idol\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc162cbb2e8049448bc3ce4c9b498020",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Images:   0%|          | 0/396 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting images for Jason Derulo\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e585660d00742adbe2a6778ff977458",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Images:   0%|          | 0/508 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATASET COMPILED!\n"
     ]
    }
   ],
   "source": [
    "ds = prepare_dataset(samples,train_ds_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3145, 2)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = np.asarray(ds, dtype=object)\n",
    "ds.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean all non recognised images from dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds = [(image,label) for (image,label) in ds if image.__class__.__name__ != 'NoneType' ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2599, 2)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = np.asarray(ds, dtype=object)\n",
    "ds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "images = [image for image,label in ds]\n",
    "labels = [label for image,label in ds]\n",
    "\n",
    "# encode class values as integers\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(labels)\n",
    "encoded_Y = encoder.transform(labels)\n",
    "# convert integers to dummy variables (i.e. one hot encoded)\n",
    "dummy_y = tf.keras.utils.to_categorical(encoded_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SPLIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(images, dummy_y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess for xception model and prepare batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices((X_train,y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = tf.data.Dataset.from_tensor_slices((X_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(image,label):\n",
    "    image = img_final = tf.cast(image, tf.float32) / 255.0\n",
    "    final_image = tf.keras.applications.xception.preprocess_input(image)\n",
    "    return final_image,label\n",
    "\n",
    "batch_size = 4\n",
    "train_dataset = train_dataset.shuffle(len(train_dataset)*2)\n",
    "train_dataset = train_dataset.map(preprocess).batch(batch_size).prefetch(1)\n",
    "\n",
    "test_dataset = test_dataset.shuffle(len(test_dataset)*2)\n",
    "test_dataset = test_dataset.map(preprocess).batch(batch_size).prefetch(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PREPROCESS DATASET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BUILDING THE MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use Transfer Learning to build on top of an Xception model architecture for image classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_8\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_9 (InputLayer)            [(None, 160, 160, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "block1_conv1 (Conv2D)           (None, 79, 79, 32)   864         input_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "block1_conv1_bn (BatchNormaliza (None, 79, 79, 32)   128         block1_conv1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block1_conv1_act (Activation)   (None, 79, 79, 32)   0           block1_conv1_bn[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block1_conv2 (Conv2D)           (None, 77, 77, 64)   18432       block1_conv1_act[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block1_conv2_bn (BatchNormaliza (None, 77, 77, 64)   256         block1_conv2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block1_conv2_act (Activation)   (None, 77, 77, 64)   0           block1_conv2_bn[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block2_sepconv1 (SeparableConv2 (None, 77, 77, 128)  8768        block1_conv2_act[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block2_sepconv1_bn (BatchNormal (None, 77, 77, 128)  512         block2_sepconv1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block2_sepconv2_act (Activation (None, 77, 77, 128)  0           block2_sepconv1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block2_sepconv2 (SeparableConv2 (None, 77, 77, 128)  17536       block2_sepconv2_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block2_sepconv2_bn (BatchNormal (None, 77, 77, 128)  512         block2_sepconv2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_32 (Conv2D)              (None, 39, 39, 128)  8192        block1_conv2_act[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block2_pool (MaxPooling2D)      (None, 39, 39, 128)  0           block2_sepconv2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_32 (BatchNo (None, 39, 39, 128)  512         conv2d_32[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_96 (Add)                    (None, 39, 39, 128)  0           block2_pool[0][0]                \n",
      "                                                                 batch_normalization_32[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "block3_sepconv1_act (Activation (None, 39, 39, 128)  0           add_96[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "block3_sepconv1 (SeparableConv2 (None, 39, 39, 256)  33920       block3_sepconv1_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block3_sepconv1_bn (BatchNormal (None, 39, 39, 256)  1024        block3_sepconv1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block3_sepconv2_act (Activation (None, 39, 39, 256)  0           block3_sepconv1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block3_sepconv2 (SeparableConv2 (None, 39, 39, 256)  67840       block3_sepconv2_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block3_sepconv2_bn (BatchNormal (None, 39, 39, 256)  1024        block3_sepconv2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_33 (Conv2D)              (None, 20, 20, 256)  32768       add_96[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "block3_pool (MaxPooling2D)      (None, 20, 20, 256)  0           block3_sepconv2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_33 (BatchNo (None, 20, 20, 256)  1024        conv2d_33[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_97 (Add)                    (None, 20, 20, 256)  0           block3_pool[0][0]                \n",
      "                                                                 batch_normalization_33[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "block4_sepconv1_act (Activation (None, 20, 20, 256)  0           add_97[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "block4_sepconv1 (SeparableConv2 (None, 20, 20, 728)  188672      block4_sepconv1_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block4_sepconv1_bn (BatchNormal (None, 20, 20, 728)  2912        block4_sepconv1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block4_sepconv2_act (Activation (None, 20, 20, 728)  0           block4_sepconv1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block4_sepconv2 (SeparableConv2 (None, 20, 20, 728)  536536      block4_sepconv2_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block4_sepconv2_bn (BatchNormal (None, 20, 20, 728)  2912        block4_sepconv2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_34 (Conv2D)              (None, 10, 10, 728)  186368      add_97[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "block4_pool (MaxPooling2D)      (None, 10, 10, 728)  0           block4_sepconv2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_34 (BatchNo (None, 10, 10, 728)  2912        conv2d_34[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_98 (Add)                    (None, 10, 10, 728)  0           block4_pool[0][0]                \n",
      "                                                                 batch_normalization_34[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "block5_sepconv1_act (Activation (None, 10, 10, 728)  0           add_98[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "block5_sepconv1 (SeparableConv2 (None, 10, 10, 728)  536536      block5_sepconv1_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block5_sepconv1_bn (BatchNormal (None, 10, 10, 728)  2912        block5_sepconv1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block5_sepconv2_act (Activation (None, 10, 10, 728)  0           block5_sepconv1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block5_sepconv2 (SeparableConv2 (None, 10, 10, 728)  536536      block5_sepconv2_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block5_sepconv2_bn (BatchNormal (None, 10, 10, 728)  2912        block5_sepconv2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block5_sepconv3_act (Activation (None, 10, 10, 728)  0           block5_sepconv2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block5_sepconv3 (SeparableConv2 (None, 10, 10, 728)  536536      block5_sepconv3_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block5_sepconv3_bn (BatchNormal (None, 10, 10, 728)  2912        block5_sepconv3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "add_99 (Add)                    (None, 10, 10, 728)  0           block5_sepconv3_bn[0][0]         \n",
      "                                                                 add_98[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "block6_sepconv1_act (Activation (None, 10, 10, 728)  0           add_99[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "block6_sepconv1 (SeparableConv2 (None, 10, 10, 728)  536536      block6_sepconv1_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block6_sepconv1_bn (BatchNormal (None, 10, 10, 728)  2912        block6_sepconv1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block6_sepconv2_act (Activation (None, 10, 10, 728)  0           block6_sepconv1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block6_sepconv2 (SeparableConv2 (None, 10, 10, 728)  536536      block6_sepconv2_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block6_sepconv2_bn (BatchNormal (None, 10, 10, 728)  2912        block6_sepconv2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block6_sepconv3_act (Activation (None, 10, 10, 728)  0           block6_sepconv2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block6_sepconv3 (SeparableConv2 (None, 10, 10, 728)  536536      block6_sepconv3_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block6_sepconv3_bn (BatchNormal (None, 10, 10, 728)  2912        block6_sepconv3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "add_100 (Add)                   (None, 10, 10, 728)  0           block6_sepconv3_bn[0][0]         \n",
      "                                                                 add_99[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "block7_sepconv1_act (Activation (None, 10, 10, 728)  0           add_100[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "block7_sepconv1 (SeparableConv2 (None, 10, 10, 728)  536536      block7_sepconv1_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block7_sepconv1_bn (BatchNormal (None, 10, 10, 728)  2912        block7_sepconv1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block7_sepconv2_act (Activation (None, 10, 10, 728)  0           block7_sepconv1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block7_sepconv2 (SeparableConv2 (None, 10, 10, 728)  536536      block7_sepconv2_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block7_sepconv2_bn (BatchNormal (None, 10, 10, 728)  2912        block7_sepconv2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block7_sepconv3_act (Activation (None, 10, 10, 728)  0           block7_sepconv2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block7_sepconv3 (SeparableConv2 (None, 10, 10, 728)  536536      block7_sepconv3_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block7_sepconv3_bn (BatchNormal (None, 10, 10, 728)  2912        block7_sepconv3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "add_101 (Add)                   (None, 10, 10, 728)  0           block7_sepconv3_bn[0][0]         \n",
      "                                                                 add_100[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "block8_sepconv1_act (Activation (None, 10, 10, 728)  0           add_101[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "block8_sepconv1 (SeparableConv2 (None, 10, 10, 728)  536536      block8_sepconv1_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block8_sepconv1_bn (BatchNormal (None, 10, 10, 728)  2912        block8_sepconv1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block8_sepconv2_act (Activation (None, 10, 10, 728)  0           block8_sepconv1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block8_sepconv2 (SeparableConv2 (None, 10, 10, 728)  536536      block8_sepconv2_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block8_sepconv2_bn (BatchNormal (None, 10, 10, 728)  2912        block8_sepconv2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block8_sepconv3_act (Activation (None, 10, 10, 728)  0           block8_sepconv2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block8_sepconv3 (SeparableConv2 (None, 10, 10, 728)  536536      block8_sepconv3_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block8_sepconv3_bn (BatchNormal (None, 10, 10, 728)  2912        block8_sepconv3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "add_102 (Add)                   (None, 10, 10, 728)  0           block8_sepconv3_bn[0][0]         \n",
      "                                                                 add_101[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "block9_sepconv1_act (Activation (None, 10, 10, 728)  0           add_102[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "block9_sepconv1 (SeparableConv2 (None, 10, 10, 728)  536536      block9_sepconv1_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block9_sepconv1_bn (BatchNormal (None, 10, 10, 728)  2912        block9_sepconv1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block9_sepconv2_act (Activation (None, 10, 10, 728)  0           block9_sepconv1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block9_sepconv2 (SeparableConv2 (None, 10, 10, 728)  536536      block9_sepconv2_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block9_sepconv2_bn (BatchNormal (None, 10, 10, 728)  2912        block9_sepconv2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block9_sepconv3_act (Activation (None, 10, 10, 728)  0           block9_sepconv2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block9_sepconv3 (SeparableConv2 (None, 10, 10, 728)  536536      block9_sepconv3_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block9_sepconv3_bn (BatchNormal (None, 10, 10, 728)  2912        block9_sepconv3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "add_103 (Add)                   (None, 10, 10, 728)  0           block9_sepconv3_bn[0][0]         \n",
      "                                                                 add_102[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "block10_sepconv1_act (Activatio (None, 10, 10, 728)  0           add_103[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "block10_sepconv1 (SeparableConv (None, 10, 10, 728)  536536      block10_sepconv1_act[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block10_sepconv1_bn (BatchNorma (None, 10, 10, 728)  2912        block10_sepconv1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block10_sepconv2_act (Activatio (None, 10, 10, 728)  0           block10_sepconv1_bn[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block10_sepconv2 (SeparableConv (None, 10, 10, 728)  536536      block10_sepconv2_act[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block10_sepconv2_bn (BatchNorma (None, 10, 10, 728)  2912        block10_sepconv2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block10_sepconv3_act (Activatio (None, 10, 10, 728)  0           block10_sepconv2_bn[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block10_sepconv3 (SeparableConv (None, 10, 10, 728)  536536      block10_sepconv3_act[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block10_sepconv3_bn (BatchNorma (None, 10, 10, 728)  2912        block10_sepconv3[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "add_104 (Add)                   (None, 10, 10, 728)  0           block10_sepconv3_bn[0][0]        \n",
      "                                                                 add_103[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "block11_sepconv1_act (Activatio (None, 10, 10, 728)  0           add_104[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "block11_sepconv1 (SeparableConv (None, 10, 10, 728)  536536      block11_sepconv1_act[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block11_sepconv1_bn (BatchNorma (None, 10, 10, 728)  2912        block11_sepconv1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block11_sepconv2_act (Activatio (None, 10, 10, 728)  0           block11_sepconv1_bn[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block11_sepconv2 (SeparableConv (None, 10, 10, 728)  536536      block11_sepconv2_act[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block11_sepconv2_bn (BatchNorma (None, 10, 10, 728)  2912        block11_sepconv2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block11_sepconv3_act (Activatio (None, 10, 10, 728)  0           block11_sepconv2_bn[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block11_sepconv3 (SeparableConv (None, 10, 10, 728)  536536      block11_sepconv3_act[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block11_sepconv3_bn (BatchNorma (None, 10, 10, 728)  2912        block11_sepconv3[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "add_105 (Add)                   (None, 10, 10, 728)  0           block11_sepconv3_bn[0][0]        \n",
      "                                                                 add_104[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "block12_sepconv1_act (Activatio (None, 10, 10, 728)  0           add_105[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "block12_sepconv1 (SeparableConv (None, 10, 10, 728)  536536      block12_sepconv1_act[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block12_sepconv1_bn (BatchNorma (None, 10, 10, 728)  2912        block12_sepconv1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block12_sepconv2_act (Activatio (None, 10, 10, 728)  0           block12_sepconv1_bn[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block12_sepconv2 (SeparableConv (None, 10, 10, 728)  536536      block12_sepconv2_act[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block12_sepconv2_bn (BatchNorma (None, 10, 10, 728)  2912        block12_sepconv2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block12_sepconv3_act (Activatio (None, 10, 10, 728)  0           block12_sepconv2_bn[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block12_sepconv3 (SeparableConv (None, 10, 10, 728)  536536      block12_sepconv3_act[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block12_sepconv3_bn (BatchNorma (None, 10, 10, 728)  2912        block12_sepconv3[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "add_106 (Add)                   (None, 10, 10, 728)  0           block12_sepconv3_bn[0][0]        \n",
      "                                                                 add_105[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "block13_sepconv1_act (Activatio (None, 10, 10, 728)  0           add_106[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "block13_sepconv1 (SeparableConv (None, 10, 10, 728)  536536      block13_sepconv1_act[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block13_sepconv1_bn (BatchNorma (None, 10, 10, 728)  2912        block13_sepconv1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block13_sepconv2_act (Activatio (None, 10, 10, 728)  0           block13_sepconv1_bn[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block13_sepconv2 (SeparableConv (None, 10, 10, 1024) 752024      block13_sepconv2_act[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block13_sepconv2_bn (BatchNorma (None, 10, 10, 1024) 4096        block13_sepconv2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_35 (Conv2D)              (None, 5, 5, 1024)   745472      add_106[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "block13_pool (MaxPooling2D)     (None, 5, 5, 1024)   0           block13_sepconv2_bn[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_35 (BatchNo (None, 5, 5, 1024)   4096        conv2d_35[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_107 (Add)                   (None, 5, 5, 1024)   0           block13_pool[0][0]               \n",
      "                                                                 batch_normalization_35[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "block14_sepconv1 (SeparableConv (None, 5, 5, 1536)   1582080     add_107[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "block14_sepconv1_bn (BatchNorma (None, 5, 5, 1536)   6144        block14_sepconv1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block14_sepconv1_act (Activatio (None, 5, 5, 1536)   0           block14_sepconv1_bn[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block14_sepconv2 (SeparableConv (None, 5, 5, 2048)   3159552     block14_sepconv1_act[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block14_sepconv2_bn (BatchNorma (None, 5, 5, 2048)   8192        block14_sepconv2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block14_sepconv2_act (Activatio (None, 5, 5, 2048)   0           block14_sepconv2_bn[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_8 (Glo (None, 2048)         0           block14_sepconv2_act[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 6)            12294       global_average_pooling2d_8[0][0] \n",
      "==================================================================================================\n",
      "Total params: 20,873,774\n",
      "Trainable params: 12,294\n",
      "Non-trainable params: 20,861,480\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "n_classes = len(list(samples.keys()))\n",
    "\n",
    "base_model = keras.applications.Xception(\n",
    "    weights=\"imagenet\",  # Load weights pre-trained on ImageNet.\n",
    "    input_shape=(160, 160, 3),\n",
    "    include_top=False\n",
    ")  # Do not include the ImageNet classifier at the top.\n",
    "\n",
    "\n",
    "avg = keras.layers.GlobalAveragePooling2D()(base_model.output)\n",
    "# drop = keras.layers.Dropout(0.2)(avg)  # Regularize with dropout\n",
    "output = keras.layers.Dense(n_classes, activation='softmax')(avg)\n",
    "model = keras.Model(base_model.input, output)\n",
    "\n",
    "# Freeze the base_model\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "455/455 [==============================] - 17s 30ms/step - loss: 1.9853 - accuracy: 0.2848 - val_loss: 1.4426 - val_accuracy: 0.4372\n",
      "Epoch 2/200\n",
      "455/455 [==============================] - 13s 27ms/step - loss: 1.6845 - accuracy: 0.3964 - val_loss: 1.8393 - val_accuracy: 0.3462\n",
      "Epoch 3/200\n",
      "455/455 [==============================] - 13s 27ms/step - loss: 1.5709 - accuracy: 0.4426 - val_loss: 1.6275 - val_accuracy: 0.3859\n",
      "Epoch 4/200\n",
      "455/455 [==============================] - 12s 27ms/step - loss: 1.5243 - accuracy: 0.4535 - val_loss: 2.0337 - val_accuracy: 0.3923\n",
      "Epoch 5/200\n",
      "455/455 [==============================] - 13s 28ms/step - loss: 1.5348 - accuracy: 0.4805 - val_loss: 1.6703 - val_accuracy: 0.4244\n",
      "Epoch 6/200\n",
      "455/455 [==============================] - 13s 28ms/step - loss: 1.2692 - accuracy: 0.5459 - val_loss: 1.5589 - val_accuracy: 0.4346\n",
      "Epoch 7/200\n",
      "455/455 [==============================] - 13s 28ms/step - loss: 1.3461 - accuracy: 0.5261 - val_loss: 1.6431 - val_accuracy: 0.4474\n",
      "Epoch 8/200\n",
      "455/455 [==============================] - 13s 28ms/step - loss: 1.3092 - accuracy: 0.5404 - val_loss: 2.2345 - val_accuracy: 0.4795\n",
      "Epoch 9/200\n",
      "455/455 [==============================] - 13s 28ms/step - loss: 1.2042 - accuracy: 0.5756 - val_loss: 1.2389 - val_accuracy: 0.5718\n",
      "Epoch 10/200\n",
      "455/455 [==============================] - 12s 27ms/step - loss: 1.1406 - accuracy: 0.5800 - val_loss: 1.2083 - val_accuracy: 0.5795\n",
      "Epoch 11/200\n",
      "455/455 [==============================] - 13s 28ms/step - loss: 1.1895 - accuracy: 0.5717 - val_loss: 1.9892 - val_accuracy: 0.4462\n",
      "Epoch 12/200\n",
      "455/455 [==============================] - 12s 27ms/step - loss: 1.1199 - accuracy: 0.5888 - val_loss: 1.4254 - val_accuracy: 0.5526\n",
      "Epoch 13/200\n",
      "455/455 [==============================] - 13s 27ms/step - loss: 1.1855 - accuracy: 0.5992 - val_loss: 1.1806 - val_accuracy: 0.5615\n",
      "Epoch 14/200\n",
      "455/455 [==============================] - 13s 27ms/step - loss: 1.1389 - accuracy: 0.6014 - val_loss: 2.0542 - val_accuracy: 0.5026\n",
      "Epoch 15/200\n",
      "455/455 [==============================] - 13s 28ms/step - loss: 1.0902 - accuracy: 0.6097 - val_loss: 1.3562 - val_accuracy: 0.5756\n",
      "Epoch 16/200\n",
      "455/455 [==============================] - 13s 27ms/step - loss: 1.0927 - accuracy: 0.6168 - val_loss: 1.3292 - val_accuracy: 0.5615\n",
      "Epoch 17/200\n",
      "455/455 [==============================] - 13s 27ms/step - loss: 1.0295 - accuracy: 0.6251 - val_loss: 1.3909 - val_accuracy: 0.5756\n",
      "Epoch 18/200\n",
      "455/455 [==============================] - 13s 28ms/step - loss: 1.0854 - accuracy: 0.6190 - val_loss: 1.4409 - val_accuracy: 0.5154s: 1.0796 - accura\n",
      "Epoch 19/200\n",
      "455/455 [==============================] - 13s 28ms/step - loss: 1.0262 - accuracy: 0.6372 - val_loss: 1.5234 - val_accuracy: 0.5372\n",
      "Epoch 20/200\n",
      "455/455 [==============================] - 13s 28ms/step - loss: 1.1403 - accuracy: 0.6196 - val_loss: 1.2934 - val_accuracy: 0.5808\n",
      "Epoch 21/200\n",
      "455/455 [==============================] - 13s 28ms/step - loss: 1.0060 - accuracy: 0.6520 - val_loss: 1.3814 - val_accuracy: 0.5500\n",
      "Epoch 22/200\n",
      "455/455 [==============================] - 13s 27ms/step - loss: 1.0929 - accuracy: 0.6185 - val_loss: 1.1122 - val_accuracy: 0.6667\n",
      "Epoch 23/200\n",
      "455/455 [==============================] - 13s 28ms/step - loss: 0.9360 - accuracy: 0.6745 - val_loss: 1.4010 - val_accuracy: 0.5756\n",
      "Epoch 24/200\n",
      "455/455 [==============================] - 13s 27ms/step - loss: 1.0148 - accuracy: 0.6509 - val_loss: 1.4503 - val_accuracy: 0.5654\n",
      "Epoch 25/200\n",
      "455/455 [==============================] - 13s 27ms/step - loss: 1.0764 - accuracy: 0.6311 - val_loss: 1.5318 - val_accuracy: 0.5551\n",
      "Epoch 26/200\n",
      "455/455 [==============================] - 13s 28ms/step - loss: 1.0478 - accuracy: 0.6526 - val_loss: 1.4309 - val_accuracy: 0.5628\n",
      "Epoch 27/200\n",
      "455/455 [==============================] - 13s 27ms/step - loss: 0.9154 - accuracy: 0.6850 - val_loss: 1.6073 - val_accuracy: 0.5282\n",
      "Epoch 28/200\n",
      "455/455 [==============================] - 12s 27ms/step - loss: 0.9466 - accuracy: 0.6696 - val_loss: 2.6380 - val_accuracy: 0.4423\n",
      "Epoch 29/200\n",
      "455/455 [==============================] - 13s 27ms/step - loss: 1.0385 - accuracy: 0.6641 - val_loss: 1.1200 - val_accuracy: 0.6436\n",
      "Epoch 30/200\n",
      "455/455 [==============================] - 13s 27ms/step - loss: 1.0175 - accuracy: 0.6592 - val_loss: 1.9155 - val_accuracy: 0.6038\n",
      "Epoch 31/200\n",
      "455/455 [==============================] - 12s 27ms/step - loss: 0.8735 - accuracy: 0.6921 - val_loss: 1.1623 - val_accuracy: 0.6487\n",
      "Epoch 32/200\n",
      "455/455 [==============================] - 12s 27ms/step - loss: 1.0140 - accuracy: 0.6586 - val_loss: 1.3165 - val_accuracy: 0.6167\n",
      "Epoch 33/200\n",
      "455/455 [==============================] - 13s 27ms/step - loss: 0.9946 - accuracy: 0.6762 - val_loss: 1.4710 - val_accuracy: 0.5897\n",
      "Epoch 34/200\n",
      "455/455 [==============================] - 13s 28ms/step - loss: 0.9295 - accuracy: 0.6872 - val_loss: 1.3670 - val_accuracy: 0.6013\n",
      "Epoch 35/200\n",
      "455/455 [==============================] - 12s 27ms/step - loss: 0.8882 - accuracy: 0.6976 - val_loss: 2.0873 - val_accuracy: 0.5538\n",
      "Epoch 36/200\n",
      "455/455 [==============================] - 12s 27ms/step - loss: 0.9363 - accuracy: 0.6855 - val_loss: 1.1071 - val_accuracy: 0.6577\n",
      "Epoch 37/200\n",
      "455/455 [==============================] - 13s 28ms/step - loss: 0.8719 - accuracy: 0.6960 - val_loss: 1.5587 - val_accuracy: 0.5359\n",
      "Epoch 38/200\n",
      "455/455 [==============================] - 13s 27ms/step - loss: 0.8548 - accuracy: 0.7037 - val_loss: 1.6855 - val_accuracy: 0.5897\n",
      "Epoch 39/200\n",
      "455/455 [==============================] - 13s 28ms/step - loss: 0.8601 - accuracy: 0.6899 - val_loss: 1.1991 - val_accuracy: 0.6436\n",
      "Epoch 40/200\n",
      "455/455 [==============================] - 13s 27ms/step - loss: 0.8906 - accuracy: 0.7009 - val_loss: 1.1620 - val_accuracy: 0.6487\n",
      "Epoch 41/200\n",
      "455/455 [==============================] - 12s 27ms/step - loss: 0.8229 - accuracy: 0.6971 - val_loss: 1.4252 - val_accuracy: 0.5654\n",
      "Epoch 42/200\n",
      "455/455 [==============================] - 13s 28ms/step - loss: 0.8431 - accuracy: 0.7174 - val_loss: 1.2666 - val_accuracy: 0.6128\n",
      "Epoch 43/200\n",
      "455/455 [==============================] - 12s 27ms/step - loss: 0.8544 - accuracy: 0.7125 - val_loss: 1.8680 - val_accuracy: 0.5064\n",
      "Epoch 44/200\n",
      "455/455 [==============================] - 12s 27ms/step - loss: 0.8880 - accuracy: 0.6916 - val_loss: 1.9811 - val_accuracy: 0.5026\n",
      "Epoch 45/200\n",
      "455/455 [==============================] - 12s 27ms/step - loss: 0.8668 - accuracy: 0.7147 - val_loss: 2.0000 - val_accuracy: 0.4846\n",
      "Epoch 46/200\n",
      "455/455 [==============================] - 12s 27ms/step - loss: 0.8087 - accuracy: 0.7147 - val_loss: 1.1478 - val_accuracy: 0.6654\n",
      "Epoch 47/200\n",
      "455/455 [==============================] - 13s 27ms/step - loss: 0.8448 - accuracy: 0.7097 - val_loss: 1.4295 - val_accuracy: 0.6179\n",
      "Epoch 48/200\n",
      "455/455 [==============================] - 13s 27ms/step - loss: 0.9032 - accuracy: 0.7218 - val_loss: 1.3241 - val_accuracy: 0.6244\n",
      "Epoch 49/200\n",
      "455/455 [==============================] - 13s 27ms/step - loss: 0.8076 - accuracy: 0.7224 - val_loss: 1.3956 - val_accuracy: 0.6372\n",
      "Epoch 50/200\n",
      "455/455 [==============================] - 13s 28ms/step - loss: 0.8093 - accuracy: 0.7130 - val_loss: 1.7822 - val_accuracy: 0.5590\n",
      "Epoch 51/200\n",
      "455/455 [==============================] - 12s 27ms/step - loss: 0.8114 - accuracy: 0.7185 - val_loss: 1.5081 - val_accuracy: 0.5872\n",
      "Epoch 52/200\n",
      "455/455 [==============================] - 12s 27ms/step - loss: 0.8058 - accuracy: 0.7202 - val_loss: 1.2926 - val_accuracy: 0.6436\n",
      "Epoch 53/200\n",
      "455/455 [==============================] - 13s 28ms/step - loss: 0.8021 - accuracy: 0.7372 - val_loss: 1.4368 - val_accuracy: 0.5846\n",
      "Epoch 54/200\n",
      "455/455 [==============================] - 13s 27ms/step - loss: 0.8346 - accuracy: 0.7257 - val_loss: 1.1723 - val_accuracy: 0.6679\n",
      "Epoch 55/200\n",
      "455/455 [==============================] - 13s 28ms/step - loss: 0.7853 - accuracy: 0.7334 - val_loss: 1.4941 - val_accuracy: 0.6013\n",
      "Epoch 56/200\n",
      "455/455 [==============================] - 13s 28ms/step - loss: 0.8407 - accuracy: 0.7328 - val_loss: 2.0965 - val_accuracy: 0.5641\n",
      "Epoch 57/200\n",
      "455/455 [==============================] - 13s 28ms/step - loss: 0.7693 - accuracy: 0.7405 - val_loss: 1.3725 - val_accuracy: 0.6231\n",
      "Epoch 58/200\n",
      "455/455 [==============================] - 13s 28ms/step - loss: 0.7709 - accuracy: 0.7301 - val_loss: 1.5181 - val_accuracy: 0.5667\n",
      "Epoch 59/200\n",
      "455/455 [==============================] - 13s 28ms/step - loss: 0.7955 - accuracy: 0.7279 - val_loss: 1.8408 - val_accuracy: 0.5346\n",
      "Epoch 60/200\n",
      "455/455 [==============================] - 13s 28ms/step - loss: 0.7152 - accuracy: 0.7548 - val_loss: 1.6926 - val_accuracy: 0.5551\n",
      "Epoch 61/200\n",
      "455/455 [==============================] - 13s 28ms/step - loss: 0.7270 - accuracy: 0.7532 - val_loss: 1.2875 - val_accuracy: 0.6487\n",
      "Epoch 62/200\n",
      "455/455 [==============================] - 13s 28ms/step - loss: 0.7374 - accuracy: 0.7455 - val_loss: 1.6772 - val_accuracy: 0.6051\n",
      "Epoch 63/200\n",
      "455/455 [==============================] - 13s 27ms/step - loss: 0.7982 - accuracy: 0.7367 - val_loss: 1.2694 - val_accuracy: 0.6436\n",
      "Epoch 64/200\n",
      "455/455 [==============================] - 12s 27ms/step - loss: 0.7838 - accuracy: 0.7422 - val_loss: 1.9650 - val_accuracy: 0.5269\n",
      "Epoch 65/200\n",
      "455/455 [==============================] - 13s 27ms/step - loss: 0.7046 - accuracy: 0.7625 - val_loss: 1.5237 - val_accuracy: 0.5962\n",
      "Epoch 66/200\n",
      "455/455 [==============================] - 13s 27ms/step - loss: 0.7799 - accuracy: 0.7422 - val_loss: 1.3693 - val_accuracy: 0.6410\n",
      "Epoch 67/200\n",
      "455/455 [==============================] - 12s 27ms/step - loss: 0.8293 - accuracy: 0.7185 - val_loss: 1.6153 - val_accuracy: 0.5718\n",
      "Epoch 68/200\n",
      "455/455 [==============================] - 12s 27ms/step - loss: 0.7522 - accuracy: 0.7422 - val_loss: 1.3694 - val_accuracy: 0.6333\n",
      "Epoch 69/200\n",
      "455/455 [==============================] - 12s 27ms/step - loss: 0.7110 - accuracy: 0.7493 - val_loss: 1.4601 - val_accuracy: 0.6051\n",
      "Epoch 70/200\n",
      "455/455 [==============================] - 12s 27ms/step - loss: 0.7939 - accuracy: 0.7240 - val_loss: 1.4132 - val_accuracy: 0.6308\n",
      "Epoch 71/200\n",
      "455/455 [==============================] - 13s 27ms/step - loss: 0.6887 - accuracy: 0.7636 - val_loss: 1.7079 - val_accuracy: 0.5833\n",
      "Epoch 72/200\n",
      "455/455 [==============================] - 13s 27ms/step - loss: 0.7408 - accuracy: 0.7499 - val_loss: 1.4909 - val_accuracy: 0.6333\n",
      "Epoch 73/200\n",
      "455/455 [==============================] - 12s 27ms/step - loss: 0.7984 - accuracy: 0.7213 - val_loss: 1.4977 - val_accuracy: 0.5910\n",
      "Epoch 74/200\n",
      "455/455 [==============================] - 13s 27ms/step - loss: 0.7501 - accuracy: 0.7466 - val_loss: 1.7441 - val_accuracy: 0.6038\n",
      "Epoch 75/200\n",
      "455/455 [==============================] - 12s 27ms/step - loss: 0.6784 - accuracy: 0.7741 - val_loss: 1.5833 - val_accuracy: 0.5987\n",
      "Epoch 76/200\n",
      "455/455 [==============================] - 12s 27ms/step - loss: 0.7485 - accuracy: 0.7565 - val_loss: 1.5237 - val_accuracy: 0.6179\n",
      "Epoch 77/200\n",
      "455/455 [==============================] - 12s 27ms/step - loss: 0.6172 - accuracy: 0.7856 - val_loss: 1.2328 - val_accuracy: 0.6641\n",
      "Epoch 78/200\n",
      "455/455 [==============================] - 12s 27ms/step - loss: 0.8348 - accuracy: 0.7306 - val_loss: 1.9551 - val_accuracy: 0.5333\n",
      "Epoch 79/200\n",
      "455/455 [==============================] - 13s 27ms/step - loss: 0.6751 - accuracy: 0.7730 - val_loss: 1.5106 - val_accuracy: 0.6141\n",
      "Epoch 80/200\n",
      "455/455 [==============================] - 13s 28ms/step - loss: 0.6690 - accuracy: 0.7768 - val_loss: 1.3475 - val_accuracy: 0.6513\n",
      "Epoch 81/200\n",
      "455/455 [==============================] - 13s 28ms/step - loss: 0.6798 - accuracy: 0.7620 - val_loss: 1.3003 - val_accuracy: 0.6641\n",
      "Epoch 82/200\n",
      "455/455 [==============================] - 13s 28ms/step - loss: 0.6715 - accuracy: 0.7680 - val_loss: 2.0899 - val_accuracy: 0.5282\n",
      "Epoch 83/200\n",
      "455/455 [==============================] - 12s 27ms/step - loss: 0.6903 - accuracy: 0.7603 - val_loss: 1.4759 - val_accuracy: 0.6064\n",
      "Epoch 84/200\n",
      "455/455 [==============================] - 13s 27ms/step - loss: 0.7331 - accuracy: 0.7598 - val_loss: 1.3903 - val_accuracy: 0.6333\n",
      "Epoch 85/200\n",
      "455/455 [==============================] - 12s 27ms/step - loss: 0.6198 - accuracy: 0.7795 - val_loss: 1.7306 - val_accuracy: 0.5962\n",
      "Epoch 86/200\n",
      "455/455 [==============================] - 13s 27ms/step - loss: 0.6510 - accuracy: 0.7784 - val_loss: 1.2605 - val_accuracy: 0.6718\n",
      "Epoch 87/200\n",
      "455/455 [==============================] - 13s 27ms/step - loss: 0.6818 - accuracy: 0.7658 - val_loss: 1.6444 - val_accuracy: 0.6064\n",
      "Epoch 88/200\n",
      "455/455 [==============================] - 13s 28ms/step - loss: 0.6251 - accuracy: 0.7801 - val_loss: 1.8218 - val_accuracy: 0.5705\n",
      "Epoch 89/200\n",
      "455/455 [==============================] - 13s 27ms/step - loss: 0.6958 - accuracy: 0.7625 - val_loss: 1.2376 - val_accuracy: 0.6705\n",
      "Epoch 90/200\n",
      "455/455 [==============================] - 12s 27ms/step - loss: 0.8122 - accuracy: 0.7466 - val_loss: 1.7814 - val_accuracy: 0.5859\n",
      "Epoch 91/200\n",
      "455/455 [==============================] - 12s 27ms/step - loss: 0.7093 - accuracy: 0.7653 - val_loss: 1.4781 - val_accuracy: 0.6256\n",
      "Epoch 92/200\n",
      "455/455 [==============================] - 12s 27ms/step - loss: 0.7394 - accuracy: 0.7515 - val_loss: 1.3068 - val_accuracy: 0.6679\n",
      "Epoch 93/200\n",
      "455/455 [==============================] - 13s 27ms/step - loss: 0.6163 - accuracy: 0.7839 - val_loss: 1.7249 - val_accuracy: 0.6141\n",
      "Epoch 94/200\n",
      "455/455 [==============================] - 13s 27ms/step - loss: 0.6756 - accuracy: 0.7784 - val_loss: 1.4493 - val_accuracy: 0.6205\n",
      "Epoch 95/200\n",
      "455/455 [==============================] - 13s 28ms/step - loss: 0.6142 - accuracy: 0.7839 - val_loss: 1.7524 - val_accuracy: 0.5718\n",
      "Epoch 96/200\n",
      "455/455 [==============================] - 12s 27ms/step - loss: 0.6218 - accuracy: 0.7812 - val_loss: 1.3685 - val_accuracy: 0.6577\n",
      "Epoch 97/200\n",
      "455/455 [==============================] - 12s 27ms/step - loss: 0.6285 - accuracy: 0.7856 - val_loss: 1.4566 - val_accuracy: 0.6436\n",
      "Epoch 98/200\n",
      "455/455 [==============================] - 13s 28ms/step - loss: 0.7091 - accuracy: 0.7713 - val_loss: 1.6688 - val_accuracy: 0.6000\n",
      "Epoch 99/200\n",
      "455/455 [==============================] - 12s 27ms/step - loss: 0.6461 - accuracy: 0.7675 - val_loss: 1.4038 - val_accuracy: 0.6526\n",
      "Epoch 100/200\n",
      "455/455 [==============================] - 12s 27ms/step - loss: 0.6033 - accuracy: 0.7883 - val_loss: 1.6693 - val_accuracy: 0.6090\n",
      "Epoch 101/200\n",
      "455/455 [==============================] - 12s 27ms/step - loss: 0.5912 - accuracy: 0.7916 - val_loss: 1.4866 - val_accuracy: 0.6436\n",
      "Epoch 102/200\n",
      "455/455 [==============================] - 13s 28ms/step - loss: 0.5900 - accuracy: 0.7971 - val_loss: 1.6624 - val_accuracy: 0.6231\n",
      "Epoch 103/200\n",
      "455/455 [==============================] - 12s 27ms/step - loss: 0.5931 - accuracy: 0.7955 - val_loss: 1.7119 - val_accuracy: 0.6026\n",
      "Epoch 104/200\n",
      "455/455 [==============================] - 13s 28ms/step - loss: 0.6965 - accuracy: 0.7817 - val_loss: 1.7260 - val_accuracy: 0.6051\n",
      "Epoch 105/200\n",
      "455/455 [==============================] - 12s 27ms/step - loss: 0.6408 - accuracy: 0.7839 - val_loss: 1.7123 - val_accuracy: 0.6231\n",
      "Epoch 106/200\n",
      "455/455 [==============================] - 12s 27ms/step - loss: 0.6417 - accuracy: 0.7746 - val_loss: 1.4174 - val_accuracy: 0.6513\n",
      "Epoch 107/200\n",
      "455/455 [==============================] - 13s 28ms/step - loss: 0.6243 - accuracy: 0.7850 - val_loss: 1.4473 - val_accuracy: 0.6577\n",
      "Epoch 108/200\n",
      "455/455 [==============================] - 13s 27ms/step - loss: 0.6098 - accuracy: 0.7927 - val_loss: 2.0434 - val_accuracy: 0.5936\n",
      "Epoch 109/200\n",
      "455/455 [==============================] - 13s 27ms/step - loss: 0.5929 - accuracy: 0.7977 - val_loss: 1.4959 - val_accuracy: 0.6551\n",
      "Epoch 110/200\n",
      "455/455 [==============================] - 13s 27ms/step - loss: 0.6226 - accuracy: 0.7982 - val_loss: 1.7053 - val_accuracy: 0.6218\n",
      "Epoch 111/200\n",
      "455/455 [==============================] - 12s 27ms/step - loss: 0.6599 - accuracy: 0.7850 - val_loss: 1.8811 - val_accuracy: 0.5987\n",
      "Epoch 112/200\n",
      "455/455 [==============================] - 13s 27ms/step - loss: 0.6368 - accuracy: 0.7938 - val_loss: 1.5670 - val_accuracy: 0.6231\n",
      "Epoch 113/200\n",
      "455/455 [==============================] - 13s 27ms/step - loss: 0.6442 - accuracy: 0.7982 - val_loss: 2.1637 - val_accuracy: 0.5487\n",
      "Epoch 114/200\n",
      "455/455 [==============================] - 12s 27ms/step - loss: 0.5633 - accuracy: 0.8076 - val_loss: 1.5844 - val_accuracy: 0.6154\n",
      "Epoch 115/200\n",
      "455/455 [==============================] - 13s 28ms/step - loss: 0.6460 - accuracy: 0.7741 - val_loss: 2.0635 - val_accuracy: 0.5577\n",
      "Epoch 116/200\n",
      "455/455 [==============================] - 13s 27ms/step - loss: 0.6362 - accuracy: 0.7768 - val_loss: 1.8219 - val_accuracy: 0.6218\n",
      "Epoch 117/200\n",
      "455/455 [==============================] - 12s 27ms/step - loss: 0.6141 - accuracy: 0.7944 - val_loss: 1.6528 - val_accuracy: 0.6231\n",
      "Epoch 118/200\n",
      "455/455 [==============================] - 12s 27ms/step - loss: 0.5727 - accuracy: 0.8004 - val_loss: 1.7083 - val_accuracy: 0.5923\n",
      "Epoch 119/200\n",
      "455/455 [==============================] - 13s 28ms/step - loss: 0.5481 - accuracy: 0.8109 - val_loss: 1.9784 - val_accuracy: 0.5872\n",
      "Epoch 120/200\n",
      "455/455 [==============================] - 12s 27ms/step - loss: 0.6242 - accuracy: 0.7905 - val_loss: 1.5092 - val_accuracy: 0.6449\n",
      "Epoch 121/200\n",
      "455/455 [==============================] - 12s 27ms/step - loss: 0.5786 - accuracy: 0.7977 - val_loss: 1.5139 - val_accuracy: 0.6282\n",
      "Epoch 122/200\n",
      "455/455 [==============================] - 12s 27ms/step - loss: 0.5831 - accuracy: 0.8076 - val_loss: 1.6013 - val_accuracy: 0.6115\n",
      "Epoch 123/200\n",
      "455/455 [==============================] - 12s 27ms/step - loss: 0.5562 - accuracy: 0.8037 - val_loss: 1.7379 - val_accuracy: 0.6359\n",
      "Epoch 124/200\n",
      "455/455 [==============================] - 12s 27ms/step - loss: 0.5592 - accuracy: 0.8158 - val_loss: 2.1611 - val_accuracy: 0.5462\n",
      "Epoch 125/200\n",
      "455/455 [==============================] - 12s 27ms/step - loss: 0.6720 - accuracy: 0.7741 - val_loss: 1.5278 - val_accuracy: 0.6462\n",
      "Epoch 126/200\n",
      "455/455 [==============================] - 12s 27ms/step - loss: 0.4932 - accuracy: 0.8120 - val_loss: 1.7018 - val_accuracy: 0.6282\n",
      "Epoch 127/200\n",
      "455/455 [==============================] - 12s 27ms/step - loss: 0.6670 - accuracy: 0.8004 - val_loss: 1.5129 - val_accuracy: 0.6385\n",
      "Epoch 128/200\n",
      "455/455 [==============================] - 12s 27ms/step - loss: 0.5926 - accuracy: 0.8098 - val_loss: 1.6091 - val_accuracy: 0.6256\n",
      "Epoch 129/200\n",
      "455/455 [==============================] - 12s 27ms/step - loss: 0.5907 - accuracy: 0.7988 - val_loss: 1.5131 - val_accuracy: 0.6513\n",
      "Epoch 130/200\n",
      "455/455 [==============================] - 12s 27ms/step - loss: 0.6392 - accuracy: 0.7900 - val_loss: 1.6616 - val_accuracy: 0.6256\n",
      "Epoch 131/200\n",
      "455/455 [==============================] - 13s 27ms/step - loss: 0.5139 - accuracy: 0.8136 - val_loss: 1.6572 - val_accuracy: 0.6192\n",
      "Epoch 132/200\n",
      "455/455 [==============================] - 13s 27ms/step - loss: 0.4768 - accuracy: 0.8246 - val_loss: 1.4221 - val_accuracy: 0.6692\n",
      "Epoch 133/200\n",
      "455/455 [==============================] - 13s 28ms/step - loss: 0.6592 - accuracy: 0.7839 - val_loss: 1.4075 - val_accuracy: 0.6795\n",
      "Epoch 134/200\n",
      "455/455 [==============================] - 13s 28ms/step - loss: 0.5151 - accuracy: 0.8246 - val_loss: 1.6061 - val_accuracy: 0.6333\n",
      "Epoch 135/200\n",
      "455/455 [==============================] - 13s 28ms/step - loss: 0.5468 - accuracy: 0.8081 - val_loss: 2.2109 - val_accuracy: 0.5526\n",
      "Epoch 136/200\n",
      "455/455 [==============================] - 12s 27ms/step - loss: 0.5844 - accuracy: 0.8015 - val_loss: 1.6419 - val_accuracy: 0.6308\n",
      "Epoch 137/200\n",
      "455/455 [==============================] - 13s 28ms/step - loss: 0.5926 - accuracy: 0.7988 - val_loss: 1.5286 - val_accuracy: 0.6410\n",
      "Epoch 138/200\n",
      "455/455 [==============================] - 12s 27ms/step - loss: 0.6292 - accuracy: 0.7999 - val_loss: 1.4063 - val_accuracy: 0.6705\n",
      "Epoch 139/200\n",
      "455/455 [==============================] - 13s 27ms/step - loss: 0.4949 - accuracy: 0.8296 - val_loss: 1.7870 - val_accuracy: 0.5962\n",
      "Epoch 140/200\n",
      "455/455 [==============================] - 12s 27ms/step - loss: 0.5694 - accuracy: 0.8103 - val_loss: 1.7296 - val_accuracy: 0.6205\n",
      "Epoch 141/200\n",
      "455/455 [==============================] - 13s 27ms/step - loss: 0.5407 - accuracy: 0.8191 - val_loss: 1.4286 - val_accuracy: 0.6654\n",
      "Epoch 142/200\n",
      "455/455 [==============================] - 12s 27ms/step - loss: 0.5369 - accuracy: 0.8059 - val_loss: 1.5015 - val_accuracy: 0.6641\n",
      "Epoch 143/200\n",
      "455/455 [==============================] - 12s 27ms/step - loss: 0.6008 - accuracy: 0.8037 - val_loss: 1.5295 - val_accuracy: 0.6551\n",
      "Epoch 144/200\n",
      "455/455 [==============================] - 12s 27ms/step - loss: 0.5399 - accuracy: 0.8164 - val_loss: 1.8153 - val_accuracy: 0.5833\n",
      "Epoch 145/200\n",
      "455/455 [==============================] - 12s 27ms/step - loss: 0.5182 - accuracy: 0.8285 - val_loss: 1.9871 - val_accuracy: 0.6308\n",
      "Epoch 146/200\n",
      "455/455 [==============================] - 12s 27ms/step - loss: 0.6214 - accuracy: 0.7977 - val_loss: 1.7112 - val_accuracy: 0.6397\n",
      "Epoch 147/200\n",
      "455/455 [==============================] - 12s 27ms/step - loss: 0.5434 - accuracy: 0.8076 - val_loss: 1.6569 - val_accuracy: 0.6346\n",
      "Epoch 148/200\n",
      "455/455 [==============================] - 12s 27ms/step - loss: 0.5496 - accuracy: 0.8186 - val_loss: 1.7527 - val_accuracy: 0.6179\n",
      "Epoch 149/200\n",
      "455/455 [==============================] - 12s 27ms/step - loss: 0.5103 - accuracy: 0.8219 - val_loss: 2.0497 - val_accuracy: 0.5949\n",
      "Epoch 150/200\n",
      "455/455 [==============================] - 12s 27ms/step - loss: 0.5800 - accuracy: 0.8037 - val_loss: 1.6330 - val_accuracy: 0.6385\n",
      "Epoch 151/200\n",
      "455/455 [==============================] - 12s 27ms/step - loss: 0.6017 - accuracy: 0.8043 - val_loss: 1.5406 - val_accuracy: 0.6641\n",
      "Epoch 152/200\n",
      "455/455 [==============================] - 13s 27ms/step - loss: 0.5830 - accuracy: 0.8026 - val_loss: 2.1783 - val_accuracy: 0.5859\n",
      "Epoch 153/200\n",
      "455/455 [==============================] - 12s 27ms/step - loss: 0.5574 - accuracy: 0.8186 - val_loss: 2.6908 - val_accuracy: 0.4962\n",
      "Epoch 154/200\n",
      "455/455 [==============================] - 12s 27ms/step - loss: 0.4652 - accuracy: 0.8367 - val_loss: 1.3999 - val_accuracy: 0.6833\n",
      "Epoch 155/200\n",
      "455/455 [==============================] - 12s 27ms/step - loss: 0.5003 - accuracy: 0.8318 - val_loss: 1.6326 - val_accuracy: 0.6269\n",
      "Epoch 156/200\n",
      "455/455 [==============================] - 12s 27ms/step - loss: 0.5032 - accuracy: 0.8252 - val_loss: 1.6622 - val_accuracy: 0.6410\n",
      "Epoch 157/200\n",
      "455/455 [==============================] - 12s 27ms/step - loss: 0.5063 - accuracy: 0.8235 - val_loss: 1.8213 - val_accuracy: 0.6205\n",
      "Epoch 158/200\n",
      "455/455 [==============================] - 12s 27ms/step - loss: 0.6538 - accuracy: 0.7933 - val_loss: 1.5478 - val_accuracy: 0.6436\n",
      "Epoch 159/200\n",
      "455/455 [==============================] - 12s 27ms/step - loss: 0.5958 - accuracy: 0.8026 - val_loss: 1.7248 - val_accuracy: 0.6385\n",
      "Epoch 160/200\n",
      "455/455 [==============================] - 12s 27ms/step - loss: 0.4775 - accuracy: 0.8439 - val_loss: 1.4955 - val_accuracy: 0.6705\n",
      "Epoch 161/200\n",
      "455/455 [==============================] - 13s 27ms/step - loss: 0.5604 - accuracy: 0.8010 - val_loss: 1.5519 - val_accuracy: 0.6513racy\n",
      "Epoch 162/200\n",
      "455/455 [==============================] - 13s 28ms/step - loss: 0.4666 - accuracy: 0.8389 - val_loss: 1.7243 - val_accuracy: 0.6308\n",
      "Epoch 163/200\n",
      "455/455 [==============================] - 13s 28ms/step - loss: 0.5125 - accuracy: 0.8241 - val_loss: 1.4397 - val_accuracy: 0.6769\n",
      "Epoch 164/200\n",
      "455/455 [==============================] - 13s 28ms/step - loss: 0.5034 - accuracy: 0.8378 - val_loss: 1.5061 - val_accuracy: 0.6705\n",
      "Epoch 165/200\n",
      "455/455 [==============================] - 12s 27ms/step - loss: 0.5359 - accuracy: 0.8235 - val_loss: 1.9199 - val_accuracy: 0.6308\n",
      "Epoch 166/200\n",
      "455/455 [==============================] - 12s 27ms/step - loss: 0.5088 - accuracy: 0.8197 - val_loss: 1.7584 - val_accuracy: 0.6385\n",
      "Epoch 167/200\n",
      "455/455 [==============================] - 13s 27ms/step - loss: 0.5136 - accuracy: 0.8285 - val_loss: 1.7321 - val_accuracy: 0.6372\n",
      "Epoch 168/200\n",
      "455/455 [==============================] - 12s 27ms/step - loss: 0.5204 - accuracy: 0.8318 - val_loss: 1.5605 - val_accuracy: 0.6590\n",
      "Epoch 169/200\n",
      "455/455 [==============================] - 12s 27ms/step - loss: 0.5257 - accuracy: 0.8169 - val_loss: 1.8548 - val_accuracy: 0.6244\n",
      "Epoch 170/200\n",
      "455/455 [==============================] - 13s 27ms/step - loss: 0.5640 - accuracy: 0.8059 - val_loss: 1.6363 - val_accuracy: 0.6538\n",
      "Epoch 171/200\n",
      "455/455 [==============================] - 13s 28ms/step - loss: 0.4568 - accuracy: 0.8345 - val_loss: 2.0979 - val_accuracy: 0.5718\n",
      "Epoch 172/200\n",
      "455/455 [==============================] - 13s 27ms/step - loss: 0.4992 - accuracy: 0.8285 - val_loss: 2.1121 - val_accuracy: 0.5756\n",
      "Epoch 173/200\n",
      "455/455 [==============================] - 13s 28ms/step - loss: 0.5171 - accuracy: 0.8301 - val_loss: 1.7649 - val_accuracy: 0.6359\n",
      "Epoch 174/200\n",
      "455/455 [==============================] - 13s 27ms/step - loss: 0.5276 - accuracy: 0.8158 - val_loss: 1.6545 - val_accuracy: 0.6359\n",
      "Epoch 175/200\n",
      "455/455 [==============================] - 12s 27ms/step - loss: 0.5613 - accuracy: 0.8070 - val_loss: 1.9277 - val_accuracy: 0.6141\n",
      "Epoch 176/200\n",
      "455/455 [==============================] - 13s 28ms/step - loss: 0.4349 - accuracy: 0.8472 - val_loss: 1.8967 - val_accuracy: 0.6038\n",
      "Epoch 177/200\n",
      "455/455 [==============================] - 13s 28ms/step - loss: 0.4966 - accuracy: 0.8395 - val_loss: 1.7701 - val_accuracy: 0.6192\n",
      "Epoch 178/200\n",
      "455/455 [==============================] - 12s 27ms/step - loss: 0.5493 - accuracy: 0.8109 - val_loss: 1.6349 - val_accuracy: 0.6577\n",
      "Epoch 179/200\n",
      "455/455 [==============================] - 13s 27ms/step - loss: 0.5559 - accuracy: 0.8147 - val_loss: 1.4918 - val_accuracy: 0.6808\n",
      "Epoch 180/200\n",
      "455/455 [==============================] - 13s 28ms/step - loss: 0.5001 - accuracy: 0.8235 - val_loss: 1.6462 - val_accuracy: 0.6628\n",
      "Epoch 181/200\n",
      "455/455 [==============================] - 12s 27ms/step - loss: 0.4861 - accuracy: 0.8307 - val_loss: 1.5910 - val_accuracy: 0.6603\n",
      "Epoch 182/200\n",
      "455/455 [==============================] - 12s 27ms/step - loss: 0.4890 - accuracy: 0.8279 - val_loss: 1.6951 - val_accuracy: 0.6513\n",
      "Epoch 183/200\n",
      "455/455 [==============================] - 13s 27ms/step - loss: 0.4223 - accuracy: 0.8560 - val_loss: 1.7673 - val_accuracy: 0.6218\n",
      "Epoch 184/200\n",
      "455/455 [==============================] - 13s 28ms/step - loss: 0.4578 - accuracy: 0.8477 - val_loss: 1.5557 - val_accuracy: 0.6731\n",
      "Epoch 185/200\n",
      "455/455 [==============================] - 13s 27ms/step - loss: 0.5248 - accuracy: 0.8246 - val_loss: 2.0108 - val_accuracy: 0.6321\n",
      "Epoch 186/200\n",
      "455/455 [==============================] - 13s 28ms/step - loss: 0.5384 - accuracy: 0.8103 - val_loss: 1.8843 - val_accuracy: 0.6346\n",
      "Epoch 187/200\n",
      "455/455 [==============================] - 13s 28ms/step - loss: 0.5033 - accuracy: 0.8301 - val_loss: 1.6771 - val_accuracy: 0.6577\n",
      "Epoch 188/200\n",
      "455/455 [==============================] - 13s 27ms/step - loss: 0.4431 - accuracy: 0.8499 - val_loss: 1.5922 - val_accuracy: 0.6769\n",
      "Epoch 189/200\n",
      "455/455 [==============================] - 13s 27ms/step - loss: 0.4910 - accuracy: 0.8285 - val_loss: 1.9162 - val_accuracy: 0.6500\n",
      "Epoch 190/200\n",
      "455/455 [==============================] - 13s 27ms/step - loss: 0.5313 - accuracy: 0.8235 - val_loss: 1.4893 - val_accuracy: 0.6859\n",
      "Epoch 191/200\n",
      "455/455 [==============================] - 13s 27ms/step - loss: 0.5096 - accuracy: 0.8263 - val_loss: 1.5093 - val_accuracy: 0.68330s - loss: 0.5147 \n",
      "Epoch 192/200\n",
      "455/455 [==============================] - 13s 27ms/step - loss: 0.4435 - accuracy: 0.8378 - val_loss: 1.9116 - val_accuracy: 0.6154\n",
      "Epoch 193/200\n",
      "455/455 [==============================] - 12s 27ms/step - loss: 0.5041 - accuracy: 0.8263 - val_loss: 1.7820 - val_accuracy: 0.6192\n",
      "Epoch 194/200\n",
      "455/455 [==============================] - 13s 28ms/step - loss: 0.5109 - accuracy: 0.8279 - val_loss: 1.5994 - val_accuracy: 0.6641\n",
      "Epoch 195/200\n",
      "455/455 [==============================] - 13s 28ms/step - loss: 0.4451 - accuracy: 0.8428 - val_loss: 1.6173 - val_accuracy: 0.6692\n",
      "Epoch 196/200\n",
      "455/455 [==============================] - 13s 28ms/step - loss: 0.5546 - accuracy: 0.8147 - val_loss: 1.8081 - val_accuracy: 0.6218\n",
      "Epoch 197/200\n",
      "455/455 [==============================] - 12s 27ms/step - loss: 0.4728 - accuracy: 0.8433 - val_loss: 1.9692 - val_accuracy: 0.5974\n",
      "Epoch 198/200\n",
      "455/455 [==============================] - 12s 27ms/step - loss: 0.4757 - accuracy: 0.8378 - val_loss: 1.5747 - val_accuracy: 0.6692\n",
      "Epoch 199/200\n",
      "455/455 [==============================] - 13s 27ms/step - loss: 0.4695 - accuracy: 0.8466 - val_loss: 1.8607 - val_accuracy: 0.6154\n",
      "Epoch 200/200\n",
      "455/455 [==============================] - 13s 28ms/step - loss: 0.4291 - accuracy: 0.8472 - val_loss: 1.6507 - val_accuracy: 0.6551\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.optimizers import Adam, SGD, Nadam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "import math\n",
    "\n",
    "initial_learning_rate = 0.2\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate,\n",
    "    decay_steps=100000,\n",
    "    decay_rate=0.96,\n",
    "    staircase=True)\n",
    "\n",
    "# optimizer = SGD(learning_rate=0.2)\n",
    "optimizer = Adam(learning_rate=lr_schedule)\n",
    "# optimizer = Nadam(0.2)\n",
    "\n",
    "# recompile\n",
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy'])\n",
    "\n",
    "# train\n",
    "history = model.fit(train_dataset,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=200,\n",
    "                    validation_data=test_dataset,\n",
    "                    verbose=1,\n",
    "                    callbacks=[\n",
    "                                ModelCheckpoint(filepath='model_ckp/model_at_ep{epoch}.h5'),\n",
    "                                    ]\n",
    ")\n",
    "model.save('model_ckp/model_final.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second round of training by manual train stopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reuse the code from above to get the model checkpoint at which validationa accuracy is at its highest and continue training until we reach the 70-80% threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "455/455 [==============================] - 16s 28ms/step - loss: 0.2234 - accuracy: 0.9269 - val_loss: 1.4712 - val_accuracy: 0.6987\n",
      "Epoch 2/50\n",
      "455/455 [==============================] - 12s 26ms/step - loss: 0.2147 - accuracy: 0.9318 - val_loss: 1.4703 - val_accuracy: 0.6987\n",
      "Epoch 3/50\n",
      "455/455 [==============================] - 12s 26ms/step - loss: 0.2136 - accuracy: 0.9340 - val_loss: 1.4717 - val_accuracy: 0.6936\n",
      "Epoch 4/50\n",
      "455/455 [==============================] - 12s 26ms/step - loss: 0.2136 - accuracy: 0.9324 - val_loss: 1.4716 - val_accuracy: 0.6936\n",
      "Epoch 5/50\n",
      "455/455 [==============================] - 12s 26ms/step - loss: 0.2137 - accuracy: 0.9335 - val_loss: 1.4706 - val_accuracy: 0.6949\n",
      "Epoch 6/50\n",
      "455/455 [==============================] - 12s 26ms/step - loss: 0.2136 - accuracy: 0.9324 - val_loss: 1.4708 - val_accuracy: 0.6949\n",
      "Epoch 7/50\n",
      "109/455 [======>.......................] - ETA: 6s - loss: 0.2127 - accuracy: 0.9312"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-60-0840953f291d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;31m# train\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m history = reloaded_model.fit(train_dataset,\n\u001b[0m\u001b[0;32m     21\u001b[0m                     \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m                     \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\AdversarialFR\\venv\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1181\u001b[0m                 _r=1):\n\u001b[0;32m   1182\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1183\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1184\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1185\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\AdversarialFR\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    887\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    891\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\AdversarialFR\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    915\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    916\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 917\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    918\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    919\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\AdversarialFR\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3021\u001b[0m       (graph_function,\n\u001b[0;32m   3022\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m-> 3023\u001b[1;33m     return graph_function._call_flat(\n\u001b[0m\u001b[0;32m   3024\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0;32m   3025\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\AdversarialFR\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1958\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1959\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1960\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1961\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1962\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32m~\\Documents\\AdversarialFR\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    589\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    590\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 591\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    592\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    593\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\AdversarialFR\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     57\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "reloaded_model = tf.keras.models.load_model('model_ckp/model_at_ep190.h5')\n",
    "\n",
    "\n",
    "initial_learning_rate = 0.02\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate,\n",
    "    decay_steps=100000,\n",
    "    decay_rate=0.9,\n",
    "    staircase=True)\n",
    "\n",
    "optimizer = SGD(learning_rate=lr_schedule)\n",
    "\n",
    "# recompile\n",
    "reloaded_model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy'])\n",
    "\n",
    "# train\n",
    "history = reloaded_model.fit(train_dataset,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=50,\n",
    "                    validation_data=test_dataset,\n",
    "                    verbose=1,\n",
    "                    callbacks=[\n",
    "                                ModelCheckpoint(filepath='model_ckp/model_at_ep{epoch}_run2.h5'),\n",
    "                                    ]\n",
    ")\n",
    "model.save('model_ckp/model_final.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fine Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "reloaded_model = tf.keras.models.load_model('model_ckp/model_at_ep190.h5')\n",
    "# unfreeze and retrain\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = True\n",
    "    \n",
    "optimizer = SGD(learning_rate=0.01, momentum=0.9, decay=0.001)\n",
    "\n",
    "# recompile\n",
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy'])\n",
    "\n",
    "# train\n",
    "history = model.fit(train_dataset,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=20,\n",
    "                    validation_data=test_dataset,\n",
    "                    verbose=1,\n",
    "                    callbacks=[\n",
    "                                ModelCheckpoint(filepath='model_FT_ckp/model_FT_at_ep{epoch}.h5'),\n",
    "                                    ]\n",
    ")\n",
    "model.save('model_FT_ckp/model_FT_final.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(acc, label='Training Accuracy')\n",
    "plt.plot(val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim([min(plt.ylim()),1])\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(loss, label='Training Loss')\n",
    "plt.plot(val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.ylabel('Cross Entropy')\n",
    "plt.ylim([0,1.0])\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for x,y in test:\n",
    "#     print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(160, 160, 3)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    C:\\Users\\Keravnos\\Documents\\AdversarialFR\\venv\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1569 predict_function  *\n        return step_function(self, iterator)\n    C:\\Users\\Keravnos\\Documents\\AdversarialFR\\venv\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1559 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    C:\\Users\\Keravnos\\Documents\\AdversarialFR\\venv\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:1285 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    C:\\Users\\Keravnos\\Documents\\AdversarialFR\\venv\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2833 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    C:\\Users\\Keravnos\\Documents\\AdversarialFR\\venv\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:3608 _call_for_each_replica\n        return fn(*args, **kwargs)\n    C:\\Users\\Keravnos\\Documents\\AdversarialFR\\venv\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1552 run_step  **\n        outputs = model.predict_step(data)\n    C:\\Users\\Keravnos\\Documents\\AdversarialFR\\venv\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1525 predict_step\n        return self(x, training=False)\n    C:\\Users\\Keravnos\\Documents\\AdversarialFR\\venv\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py:1013 __call__\n        input_spec.assert_input_compatibility(self.input_spec, inputs, self.name)\n    C:\\Users\\Keravnos\\Documents\\AdversarialFR\\venv\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\input_spec.py:267 assert_input_compatibility\n        raise ValueError('Input ' + str(input_index) +\n\n    ValueError: Input 0 is incompatible with layer model: expected shape=(None, 160, 160, 3), found shape=(160, 3)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-26-66356c4461e0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlbl\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpreprocess\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mload_test_face\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlbl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mtest_img_ds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_tensor_slices\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mpred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_img_ds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Documents\\AdversarialFR\\venv\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1725\u001b[0m           \u001b[1;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1726\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_predict_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1727\u001b[1;33m             \u001b[0mtmp_batch_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1728\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1729\u001b[0m               \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\AdversarialFR\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    887\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    891\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\AdversarialFR\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    922\u001b[0m       \u001b[1;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    923\u001b[0m       \u001b[1;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 924\u001b[1;33m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    925\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_created_variables\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    926\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
      "\u001b[1;32m~\\Documents\\AdversarialFR\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3020\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3021\u001b[0m       (graph_function,\n\u001b[1;32m-> 3022\u001b[1;33m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0m\u001b[0;32m   3023\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m   3024\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n",
      "\u001b[1;32m~\\Documents\\AdversarialFR\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   3438\u001b[0m               \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minput_signature\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3439\u001b[0m               call_context_key in self._function_cache.missed):\n\u001b[1;32m-> 3440\u001b[1;33m             return self._define_function_with_shape_relaxation(\n\u001b[0m\u001b[0;32m   3441\u001b[0m                 args, kwargs, flat_args, filtered_flat_args, cache_key_context)\n\u001b[0;32m   3442\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\AdversarialFR\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_define_function_with_shape_relaxation\u001b[1;34m(self, args, kwargs, flat_args, filtered_flat_args, cache_key_context)\u001b[0m\n\u001b[0;32m   3360\u001b[0m           expand_composites=True)\n\u001b[0;32m   3361\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3362\u001b[1;33m     graph_function = self._create_graph_function(\n\u001b[0m\u001b[0;32m   3363\u001b[0m         args, kwargs, override_flat_arg_shapes=relaxed_arg_shapes)\n\u001b[0;32m   3364\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marg_relaxed\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mrank_only_cache_key\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\AdversarialFR\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[1;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m   3277\u001b[0m     \u001b[0marg_names\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbase_arg_names\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mmissing_arg_names\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3278\u001b[0m     graph_function = ConcreteFunction(\n\u001b[1;32m-> 3279\u001b[1;33m         func_graph_module.func_graph_from_py_func(\n\u001b[0m\u001b[0;32m   3280\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3281\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_python_function\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\AdversarialFR\\venv\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[1;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m    997\u001b[0m         \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    998\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 999\u001b[1;33m       \u001b[0mfunc_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1000\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1001\u001b[0m       \u001b[1;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\AdversarialFR\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[1;34m(*args, **kwds)\u001b[0m\n\u001b[0;32m    670\u001b[0m         \u001b[1;31m# the function a weak reference to itself to avoid a reference cycle.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    671\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcompile_with_xla\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 672\u001b[1;33m           \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    673\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    674\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\AdversarialFR\\venv\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    984\u001b[0m           \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint:disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    985\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"ag_error_metadata\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 986\u001b[1;33m               \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    987\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    988\u001b[0m               \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    C:\\Users\\Keravnos\\Documents\\AdversarialFR\\venv\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1569 predict_function  *\n        return step_function(self, iterator)\n    C:\\Users\\Keravnos\\Documents\\AdversarialFR\\venv\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1559 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    C:\\Users\\Keravnos\\Documents\\AdversarialFR\\venv\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:1285 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    C:\\Users\\Keravnos\\Documents\\AdversarialFR\\venv\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2833 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    C:\\Users\\Keravnos\\Documents\\AdversarialFR\\venv\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:3608 _call_for_each_replica\n        return fn(*args, **kwargs)\n    C:\\Users\\Keravnos\\Documents\\AdversarialFR\\venv\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1552 run_step  **\n        outputs = model.predict_step(data)\n    C:\\Users\\Keravnos\\Documents\\AdversarialFR\\venv\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1525 predict_step\n        return self(x, training=False)\n    C:\\Users\\Keravnos\\Documents\\AdversarialFR\\venv\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py:1013 __call__\n        input_spec.assert_input_compatibility(self.input_spec, inputs, self.name)\n    C:\\Users\\Keravnos\\Documents\\AdversarialFR\\venv\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\input_spec.py:267 assert_input_compatibility\n        raise ValueError('Input ' + str(input_index) +\n\n    ValueError: Input 0 is incompatible with layer model: expected shape=(None, 160, 160, 3), found shape=(160, 3)\n"
     ]
    }
   ],
   "source": [
    "test_path = r'C:\\Users\\Keravnos\\Documents\\VGG-Face2\\data\\vggface2_train.tar\\train\\n002038\\0107_01.jpg'\n",
    "load_test_face = extract_faces_from_img(test_path)\n",
    "lbl = ''\n",
    "\n",
    "print(load_test_face.shape)\n",
    "img, lbl = preprocess(load_test_face,lbl)\n",
    "test_img_ds = tf.data.Dataset.from_tensor_slices(img)\n",
    "pred = model.predict(test_img_ds, batch_size=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pred' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-209-008509a38355>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'pred' is not defined"
     ]
    }
   ],
   "source": [
    "print(pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "metadata": {
   "interpreter": {
    "hash": "5152878ccfa80b2aa2c339683f5c28e34a73d20567e7e9cb69acb74956c72351"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
